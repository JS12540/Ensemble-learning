{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bagging ensembles"
      ],
      "metadata": {
        "id": "PorGUFkcmsrN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Bagging involves creating multiple samples of the training dataset.\n",
        "2. Decision trees are fitted on each sample.\n",
        "3. Variations in the training datasets lead to differences in the fitted decision trees.\n",
        "4. Predictions from individual trees are combined using simple statistics like voting or averaging.\n",
        "5. Bootstrap sampling is employed where examples (rows) are randomly drawn from the dataset with replacement.\n",
        "6. Bootstrap sampling ensures that each sample may contain duplicates of some rows.\n",
        "This process is key to the effectiveness of bagging, providing diversity in the ensemble members."
      ],
      "metadata": {
        "id": "7WYZND6DoV4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bagging\n",
        "is available in scikit-learn via the BaggingClassifier and BaggingRegressor classes, which\n",
        "use a decision tree as the base-model by default and you can specify the number of trees to\n",
        "create via the n estimators argument."
      ],
      "metadata": {
        "id": "eLmP-ve5ooph"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLMi7FSikFMk",
        "outputId": "f0e2d350-0635-4f14-aa77-b9981a4cbe7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.947 (0.081)\n"
          ]
        }
      ],
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "# create the synthetic classification dataset\n",
        "X, y = make_classification(random_state=1)\n",
        "\n",
        "# configure the ensemble model\n",
        "model = BaggingClassifier(n_estimators=50)\n",
        "\n",
        "# configure the resampling method\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "# evaluate the ensemble on the dataset using the resampling method\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "# report ensemble performance\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "changing base learner"
      ],
      "metadata": {
        "id": "REkPj3rTpZCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier  # Importing Random Forest\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "# create the synthetic classification dataset\n",
        "X, y = make_classification(random_state=1)\n",
        "\n",
        "# configure the base learner (Random Forest)\n",
        "base_learner = RandomForestClassifier()  # Using Random Forest as the base learner\n",
        "\n",
        "# configure the ensemble model with the specified base learner\n",
        "model = BaggingClassifier(base_estimator=base_learner, n_estimators=60)\n",
        "\n",
        "# configure the resampling method\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "# evaluate the ensemble on the dataset using the resampling method\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "# report ensemble performance\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYb17q6ppf1c",
        "outputId": "186b55a6-2a5e-4707-d583-7670eb5b35ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.963 (0.060)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Ensemble"
      ],
      "metadata": {
        "id": "xheto05NrOsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Random Forest is an extension of the bagging ensemble method.\n",
        "2. Similar to bagging, Random Forest fits a decision tree on various bootstrap samples of the training dataset.\n",
        "3. In addition to sampling the data, Random Forest also randomly samples the features (columns) of each dataset.\n",
        "4. When constructing each decision tree, split points are chosen in the data.\n",
        "5. Instead of considering all features for choosing a split point, Random Forest restricts the features to a random subset.\n",
        "6. For instance, if there were 10 features, Random Forest might limit the features to a subset of 3 for each split point evaluation.\n",
        "7. This feature sampling enhances the diversity among the trees in the ensemble and leads to improved generalization performance."
      ],
      "metadata": {
        "id": "knQkeEunrRwj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The random forest ensemble is available in scikit-learn via the RandomForestClassifier\n",
        "and RandomForestRegressor classes. You can specify the number of trees to create via the\n",
        "n estimators argument and the number of randomly selected features to consider at each split\n",
        "point via the max features argument, which is set to the square root of the number of features\n",
        "in your dataset by default"
      ],
      "metadata": {
        "id": "pFgphyrJrYmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# create the synthetic classification dataset\n",
        "X, y = make_classification(random_state=1)\n",
        "\n",
        "# configure the ensemble model with a specified number of trees and max_features\n",
        "model = RandomForestClassifier(n_estimators=100, max_features=\"sqrt\")  # Using the square root of the number of features as max_features\n",
        "\n",
        "# configure the resampling method\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "# evaluate the ensemble on the dataset using the resampling method\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "# report ensemble performance\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPtvUPRAqocf",
        "outputId": "b021bc58-43b7-46b3-cf1d-dae0666a9fd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.957 (0.072)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AdaBoost Ensemble"
      ],
      "metadata": {
        "id": "g6Y9dmpnr-uo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Boosting involves adding models sequentially to the ensemble where new models aim to correct errors made by prior models.\n",
        "2. The more ensemble members added, the fewer errors expected, up to a limit supported by data before overfitting.\n",
        "3. AdaBoost works by fitting decision trees on versions of the training dataset weighted so that the tree focuses more on examples that prior members got wrong.\n",
        "4. AdaBoost uses simple trees known as decision stumps, which make a single decision on one input variable before predicting."
      ],
      "metadata": {
        "id": "-OBxUwy7sA7P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaBoost is available in\n",
        "scikit-learn via the AdaBoostClassifier and AdaBoostRegressor classes, which use a decision\n",
        "tree (decision stump) as the base-model by default and you can specify the number of trees\n",
        "to create via the n estimators argument"
      ],
      "metadata": {
        "id": "SXN_N5wbuExY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# create the synthetic classification dataset\n",
        "X, y = make_classification(random_state=1)\n",
        "\n",
        "# configure the ensemble model\n",
        "model = AdaBoostClassifier(n_estimators=50)\n",
        "\n",
        "# configure the resampling method\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "# evaluate the ensemble on the dataset using the resampling method\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "# report ensemble performance\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mai6FNGt_gy",
        "outputId": "8875c1bc-273b-4b08-f081-c83eb88fc50b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.947 (0.088)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "change\n",
        "the base learner that is used (note, it must support weighted training data).\n"
      ],
      "metadata": {
        "id": "VCXq4v54uWdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# create the synthetic classification dataset\n",
        "X, y = make_classification(random_state=1)\n",
        "\n",
        "# configure the base learner (decision tree)\n",
        "base_learner = DecisionTreeClassifier(max_depth=1)\n",
        "\n",
        "# configure the ensemble model with the specified base learner and SAMME algorithm\n",
        "model = AdaBoostClassifier(base_estimator=base_learner, n_estimators=50, algorithm='SAMME')\n",
        "\n",
        "# configure the resampling method\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "# evaluate the ensemble on the dataset using the resampling method\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "# report ensemble performance\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MidTBCOduXDT",
        "outputId": "9292d711-9b06-4984-925c-f3c48eacc0a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.950 (0.081)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient Boosting Ensemble"
      ],
      "metadata": {
        "id": "jxs2DQsyyk_A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "1. Gradient boosting is a boosting ensemble algorithm that extends AdaBoost.\n",
        "2. It reframes boosting as an additive model under a statistical framework.\n",
        "3. Allows for the use of arbitrary loss functions, enhancing flexibility.\n",
        "4. Utilizes loss penalties (shrinkage) to mitigate overfitting.\n",
        "5. Introduces the concept of bagging to ensemble members, including sampling of training dataset rows and columns.\n",
        "6. Stochastic gradient boosting is a variant that involves random sampling of rows and columns during training.\n",
        "7. Particularly effective for structured or tabular data.\n",
        "Can be slow to fit due to sequential addition of models.\n",
        "8. More efficient implementations have been developed, such as:\n",
        "Extreme Gradient Boosting (XGBoost)\n",
        "Light Gradient Boosting Machines (LightGBM)"
      ],
      "metadata": {
        "id": "q8VAcarhyqL8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Gradient Boosting:\n",
        "- Gradient Boosting is an ensemble learning method used for both regression and classification tasks.\n",
        "- It builds a strong predictive model by combining multiple weak models (typically decision trees) sequentially.\n",
        "\n",
        "## Key Components:\n",
        "- **Weak Learners:** Typically decision trees with limited depth, called \"base learners\" or \"weak learners.\"\n",
        "- **Loss Function:** A loss function is optimized to minimize errors between actual and predicted values.\n",
        "- **Gradient Descent:** Errors (residuals) from the previous step are used to fit the next base learner.\n",
        "\n",
        "## Sequential Learning Process:\n",
        "- Starts with an initial weak learner that predicts the target variable.\n",
        "- Subsequent learners are trained to correct the errors made by the previous ones.\n",
        "- Each new learner focuses on minimizing the loss function with respect to the residuals of the previous model.\n",
        "\n",
        "## Boosting Mechanism:\n",
        "- Each weak learner is trained on a modified version of the data where the instances are reweighted to focus on the previously mispredicted instances.\n",
        "- The final prediction is the weighted sum of all the weak learners, where weights are determined during training.\n",
        "\n",
        "## Benefits:\n",
        "- **Improved Accuracy:** Gradient boosting often yields higher accuracy compared to individual weak learners.\n",
        "- **Handles Complex Data:** Capable of capturing complex relationships in data.\n",
        "- **Robustness:** Less prone to overfitting compared to other ensemble methods.\n",
        "- **Flexibility:** Supports various loss functions and can be customized through hyperparameters.\n",
        "\n",
        "## Popular Implementations:\n",
        "- **XGBoost:** An optimized and scalable implementation of gradient boosting.\n",
        "- **LightGBM:** A gradient boosting framework that uses tree-based learning algorithms.\n",
        "- **CatBoost:** A gradient boosting library that handles categorical features efficiently.\n",
        "- **Scikit-learn GradientBoostingRegressor/Classifier:** Part of the scikit-learn library, offering a simple interface for gradient boosting.\n"
      ],
      "metadata": {
        "id": "TSdFQImbzaiH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient boosting is available in scikit-learn via the GradientBoostingClassifier and\n",
        "GradientBoostingRegressor classes, which use a decision tree as the base-model by default.\n",
        "You can specify the number of trees to create via the n estimators argument and the learning\n",
        "rate that controls the contribution from each tree via the learning rate argument that defaults\n",
        "to 0.1."
      ],
      "metadata": {
        "id": "ZZIeAsR6y0ZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# create the synthetic classification dataset\n",
        "X, y = make_classification(random_state=1)\n",
        "\n",
        "# configure the ensemble model\n",
        "model = GradientBoostingClassifier(n_estimators=50, learning_rate=0.2)\n",
        "\n",
        "# configure the resampling method\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "# evaluate the ensemble on the dataset using the resampling method\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "# report ensemble performance\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiW6YqEEyn-D",
        "outputId": "995b79a6-9355-41bb-e89c-c24dc7e1e933"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.930 (0.100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Voting Ensemble"
      ],
      "metadata": {
        "id": "COs_IVzf1FOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Voting ensembles utilize simple statistics to combine predictions from multiple models.\n",
        "2. It involves fitting multiple different model types on the same training dataset.\n",
        "3. For regression problems, predictions are typically averaged.\n",
        "4. In classification, hard voting is used, selecting the class label with the most votes.\n",
        "5. Hard voting is effective when the base models are diverse and have different decision boundaries.\n",
        "6. Soft voting is an alternative, where predicted probabilities are summed and the label with the largest summed probability is selected.\n",
        "7. Soft voting is preferred when base models support predicting class probabilities.\n",
        "8. Soft voting can lead to better performance as it incorporates the confidence level of each model's prediction."
      ],
      "metadata": {
        "id": "U7GJwidl1G_5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voting ensembles are available in scikit-learn via the VotingClassifier and VotingRegressor\n",
        "classes.\n",
        "\n",
        "A list of base-models can be provided as an argument to the model and each model in\n",
        "the list must be a tuple with a name and the model, e.g. (‘lr’, LogisticRegression()).\n",
        "\n",
        "The\n",
        "type of voting used for classification can be specified via the voting argument and set to either\n",
        "‘soft’ or ‘hard’"
      ],
      "metadata": {
        "id": "lnxJb-JJ1T0G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# create the synthetic classification dataset\n",
        "X, y = make_classification(random_state=1)\n",
        "\n",
        "# configure the models to use in the ensemble\n",
        "models = [('lr', LogisticRegression()), ('nb', GaussianNB())]\n",
        "\n",
        "# configure the ensemble model\n",
        "model = VotingClassifier(models, voting='hard')\n",
        "\n",
        "# configure the resampling method\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "# evaluate the ensemble on the dataset using the resampling method\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "# report ensemble performance\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI6n_2Jc1N4x",
        "outputId": "34d07786-501f-4003-828e-98d0a943fbe7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.957 (0.062)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stacking Ensemble"
      ],
      "metadata": {
        "id": "hnDRlZ6H333_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Stacking combines predictions from multiple base models, similar to voting.\n",
        "2. Unlike voting, stacking employs another machine learning model, called a meta-model, to learn how to best combine the predictions of the base models.\n",
        "3. The meta-model is often a linear model like linear regression for regression tasks or logistic regression for classification tasks, but it can be any machine learning model.\n",
        "4. The meta-model is trained on the predictions made by the base models on out-of-sample data.\n",
        "5. Out-of-sample data refers to data that is not included in the training set of a machine learning model. It's used to evaluate how well the model generalizes to new, unseen observations.\n",
        "6. Out-of-sample data is generated using k-fold cross-validation for each base model, and all out-of-fold predictions are stored.\n",
        "7. Out-of-fold predictions refer to the predictions made by a machine learning model on data points that were not used during the training process for that particular fold in k-fold cross-validation.\n",
        "8. Base models are then trained on the entire training dataset.\n",
        "9. The meta-model learns from the out-of-fold predictions, determining which base models to trust, the degree of trust, and under which circumstances to trust them.\n",
        "10. Internally, stacking uses k-fold cross-validation to train the meta-model, but the evaluation of stacking models can be done using various methods such as train-test split or k-fold cross-validation.\n",
        "11. The evaluation of the model is separate from the internal resampling-for-training process.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tayQif204FoV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Stacking ensembles are available in scikit-learn via the StackingClassifier and\n",
        "StackingRegressor classes. A list of base-models can be provided as an argument to the\n",
        "model and each model in the list must be a tuple with a name and the model, e.g. (‘lr’,\n",
        "LogisticRegression()). The meta-learner can be specified via the final estimator argument\n",
        "and the resampling strategy can be specified via the cv argument and can be simply set to an\n",
        "integer indicating the number of cross-validation folds"
      ],
      "metadata": {
        "id": "Waz5b9i14VRp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# create the synthetic classification dataset\n",
        "X, y = make_classification(random_state=1)\n",
        "\n",
        "# configure the models to use in the ensemble\n",
        "models = [('knn', KNeighborsClassifier()), ('tree', DecisionTreeClassifier())]\n",
        "\n",
        "# configure the ensemble model\n",
        "model = StackingClassifier(models, final_estimator=LogisticRegression(), cv=3)\n",
        "\n",
        "# configure the resampling method\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "# evaluate the ensemble on the dataset using the resampling method\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "# report ensemble performance\n",
        "print('Mean Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vPD8h2s35eP",
        "outputId": "d627d0f9-aeff-47fc-ee10-f17506425301"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Accuracy: 0.927 (0.085)\n"
          ]
        }
      ]
    }
  ]
}